# -*- coding: utf-8 -*-
"""Hoax NB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LerYttIO1pY0rrtKVAwKmSeWWcX1qBZo

# IMPORT
"""

import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer

# MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# BernoulliNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score, classification_report

"""# READ DATA"""

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/DataMining/fake_or_real_news.csv')

"""# **HOAX NB**

## PRE-PROCESSING
"""

data.shape

data.head()

def preprocess_content(data_column):
    data_column = data_column.str.replace(r'https\S+', ' ', case=False)
    data_column = data_column.str.lower()
    data_column = data_column.str.replace(r'@\S+', ' ', case=False)
    data_column = data_column.str.replace(r'#\S+', ' ', case=False)
    data_column = data_column.str.replace("'", '')
    data_column = data_column.str.replace(r"[^\w\s]", ' ', case=False)
    data_column = data_column.str.replace(r"\s+", ' ', case=False)

    # Tokenize and apply Porter stemming
    # ps = PorterStemmer()
    # data_column = data_column.apply(lambda x: ' '.join([ps.stem(word) for word in word_tokenize(x)]))
    return data_column

data['text_stemmed'] = preprocess_content(data['text'])

# df3.drop(['unnamed: 0',	' date',	'created',	'user_id',	'followers',	'following',	'tweet_count',	'tweetlocation',	'text',	'tweet_tokens',	'tweet_tokens_wsw',	'tweet_normalized'],axis=1, inplace=True)
data['text_stemmed']

print(data['label'].value_counts())

"""## VISUALISASI"""

def plot_cloud(wordcloud):
    plt.figure(figsize=(10,8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

all_words = ' '.join([tweets for tweets in data['text_stemmed']])

wordcloud = WordCloud(
    width=3000,
    height=2000,
    random_state=3,
    background_color='black',
    colormap='Blues_r',
    collocations=False,
    stopwords=STOPWORDS
).generate(all_words)

plot_cloud(wordcloud)

plt.figure(figsize=(8, 6))
data['label'].value_counts().plot(kind='bar', color=['green', 'red'])
plt.title('Label Distribution')
plt.xlabel('label')
plt.ylabel('Total')
plt.xticks(rotation=0)
plt.show()

label_counts = data['label'].value_counts()

labels = label_counts.index
counts = label_counts.values

plt.figure(figsize=(8, 6))
plt.pie(counts, labels=labels, colors=['green', 'red'], autopct='%1.1f%%')
plt.title('Distribution Sentiment')
plt.show()

"""## NAIVE BAYES

### MULTINOMIALNB
"""

X = data['text_stemmed']
y = data['label']

vectorizer = CountVectorizer(binary=True, max_features=10000)
X_count = vectorizer.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

#MultinomialNB
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

y_pred_nb = nb_model.predict(X_test)
accuracy_nb = accuracy_score(y_train, y_pred_nb)
print("Naive Bayes Accuracy:", accuracy_nb)
print("Naive Bayes Classification Report:")
print(classification_report(y_train, y_pred_nb))

y_pred_nb = nb_model.predict(X_test)
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print("Naive Bayes Accuracy:", accuracy_nb)
print("Naive Bayes Classification Report:")
print(classification_report(y_test, y_pred_nb))

model = MultinomialNB()
param_grid = {
    'alpha': [0.2, 0.3, 0.4]
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Best Parameters:", best_params)
print("Test Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))

"""### BERNOULLINB"""

X = data['text_stemmed']
y = data['label']

vectorizer = CountVectorizer(binary=True, max_features=10000)
X_count = vectorizer.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_count, y, test_size=0.2, random_state=42)

nb_model = BernoulliNB()
nb_model.fit(X_train, y_train)

y_pred_nb = nb_model.predict(X_test)

accuracy_nb = accuracy_score(y_test, y_pred_nb)
print("Bernoulli Naive Bayes Accuracy:", accuracy_nb)
print("Bernoulli Naive Bayes Classification Report:")
print(classification_report(y_test, y_pred_nb))

model = BernoulliNB()
param_grid = {
    'alpha': [0.2]
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Best Parameters:", best_params)
print("Test Accuracy:", accuracy)
print("Classification Report:")
print(classification_report(y_test, y_pred))