# -*- coding: utf-8 -*-
"""DATA MINING

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wG_BoMPrScmLNs9STwTAJhT0Tu5lpC9E

# Import
"""

import numpy as np
import pandas as pd
import re
import string
import matplotlib.pyplot as plt
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from sklearn.utils import shuffle

import numpy as np
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard
import datetime

"""# Read Data"""

df = pd.read_csv('/content/fake_or_real_news.csv')

df.shape

df.head()

df.drop(['Unnamed: 0',	'title'],axis=1, inplace=True)

df.isna().sum()

print(df['label'].value_counts())

"""# **Pre Processing**

## Text Stemming, Tokenization, Normalization, Cleaning
"""

nltk.download('all')
# Inisialisasi stemmer dan stopwords
stemmer = PorterStemmer()
stop_words = set(stopwords.words('english'))

# Definisikan fungsi preprocessing text
def preprocess_text(text):
    # Mengubah teks menjadi huruf kecil
    text = text.lower()
    # Menghapus tautan (URL)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # Menghapus karakter non-alfanumerik
    text = re.sub(r'[^\w\s]', '', text)
    # Tokenisasi teks
    tokens = word_tokenize(text)
    # Filtering stopwords
    filtered_tokens = [word for word in tokens if word not in stop_words]
    # Stemming
    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]
    # Gabungkan kembali token yang telah diproses
    processed_text = ' '.join(stemmed_tokens)
    return processed_text

sentiments = []

# Iterasi melalui setiap baris
for index, row in df.iterrows():
    # Lakukan preprocessing pada teks dalam kolom 'tweet_raw'
    preprocessed_text = preprocess_text(row['text'])
    # Tetapkan hasil preprocessing ke kolom baru 'tweet_clean'
    df.at[index, 'text_clean'] = preprocessed_text
    # Tentukan sentimen untuk baris yang telah dipreproses

"""## Balancing"""

# Separate the data by sentiment category
real_data = df[df["label"] == "REAL"]
fake_data = df[df["label"] == "FAKE"]

# Sample 259 rows randomly from neutral_data and negative_data to balance the classes
# balanced_neutral_data = neutral_data.sample(n=259, random_state=42, replace=True)
# balanced_negative_data = negative_data.sample(n=259, random_state=42, replace=True)
balanced_real_data = real_data.sample(n=1500, random_state=42, replace=True)

# Concatenate the balanced data from each sentiment category
balanced_data = pd.concat([fake_data, balanced_real_data])

# Shuffle the data
balanced_data = balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Print the value counts to verify the balance
print(balanced_data["label"].value_counts())

balanced_data.to_csv('balance_data_news.csv', index=False)

"""## Feature Extraction"""

X = balanced_data['text_clean']
y = balanced_data['label']

"""# **Algorithm**"""

# Load IMDB dataset
max_features = 10000
maxlen = 500
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)

# Combine training and test data
x_data = np.concatenate((x_train, x_test), axis=0)
y_data = np.concatenate((y_train, y_test), axis=0)

# Shuffle the data
indices = np.arange(x_data.shape[0])
np.random.shuffle(indices)
x_data = x_data[indices]
y_data = y_data[indices]

# Calculate split index for 70% training and 30% test
split_index = int(0.7 * x_data.shape[0])

# Split the data
x_train, x_test = x_data[:split_index], x_data[split_index:]
y_train, y_test = y_data[:split_index], y_data[split_index:]

# Pad sequences
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)

# Build CNN model
embedding_dim = 50

model = Sequential()
model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

# Define callbacks
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')
early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min')

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

callbacks = [checkpoint, early_stopping, tensorboard_callback]

# Train the model
batch_size = 32
epochs = 10

history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=callbacks)

# Evaluate the model
loss, accuracy = model.evaluate(x_test, y_test)
print(f'Test Accuracy: {accuracy:.2f}')

plt.plot(history.history['accuracy'], label='train_accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()